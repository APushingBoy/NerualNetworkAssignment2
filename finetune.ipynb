{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn.modules import conv, MaxPool2d\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "#Change the stride and/or padding of some layers\n",
    "model.maxpool = MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
    "model.layer3[0].conv1 = conv.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "model.layer3[0].downsample[0] = nn.Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=Path(\"C:/Users/sclab\")/'Helperdata'\n",
    "train_dir=data_dir/'facescrub_train'\n",
    "test_dir=data_dir/'facescrub_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_name=dict()\n",
    "idx=0\n",
    "for dir in train_dir.iterdir():\n",
    "    label_to_name[idx]=dir.name\n",
    "    idx+=1\n",
    "\n",
    "label_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path:Path):\n",
    "        self.mean = np.array(mean).reshape((1, 1, 3))\n",
    "        self.std = np.array(std).reshape((1, 1, 3))\n",
    "        self.images=[]\n",
    "        for dir in path.iterdir():\n",
    "            for img in dir.rglob('*'):\n",
    "                img=str(img)\n",
    "                image=cv2.imread(img)\n",
    "                #print(type(image))\n",
    "                self.images.append(image)           \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    # Todo\n",
    "    def checktype(self):\n",
    "        print(type(self.images[0]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img=self.images[index]\n",
    "        img = img / 255.\n",
    "        img = (img - self.mean) / self.std\n",
    "        img = np.transpose(img, [2, 0, 1])\n",
    "        if len(self.images)==4000:\n",
    "            label=index//40\n",
    "        else:\n",
    "            label=index//10 \n",
    "        img = torch.tensor(img, dtype=torch.float32)\n",
    "        return img, label\n",
    "\n",
    "train_set=MyDataset(train_dir)\n",
    "test_set=MyDataset(test_dir)\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show some images\n",
    "def denorm(img):\n",
    "    for i in range(img.shape[0]):\n",
    "        img[i] = img[i] * std[i] + mean[i]\n",
    "    img = torch.clamp(img, 0., 1.)\n",
    "    return img\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    img, label = train_set[random.randint(0, len(train_set))]\n",
    "    img = denorm(img)\n",
    "    img = img.permute(1, 2, 0)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    ax.imshow(img.numpy()[:, :, ::-1])\n",
    "    ax.set_title(\"label = %d\" % label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "train_iter=DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_iter=DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "train_accs=[]\n",
    "test_losses=[]\n",
    "test_accs=[]\n",
    "\n",
    "epoches=100\n",
    "lr = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(model:nn.Module, train_iter, test_iter, num_epochs, loss_fn, mode=0):\n",
    "    if mode==0: # baseline\n",
    "        optimizer=optim.SGD(model.fc.parameters(), lr=lr, weight_decay=0.005)\n",
    "    elif mode==1: # Fine tune Conv5_x\n",
    "        optimizer=optim.SGD(model.layer4.parameters(), lr=lr, weight_decay=0.005)\n",
    "    elif mode==2: # Fine tune all the conv layers\n",
    "        for param in model.named_parameters():\n",
    "            if 'conv' not in param[0]:\n",
    "                param[1].requires_grad=False\n",
    "        optimizer=optim.SGD(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    elif mode==3: # Add 2 fc layers\n",
    "        mlp3=nn.Sequential(nn.Linear(in_features=512, out_features=256, bias=True), \n",
    "                           nn.Linear(in_features=256, out_features=128, bias=True), nn.Linear(in_features=128, out_features=100, bias=True))\n",
    "        for layer in mlp3:\n",
    "            nn.init.normal_(layer.weight)\n",
    "            nn.init.normal_(layer.bias)\n",
    "        model.fc=mlp3\n",
    "\n",
    "        for param in model.named_parameters():\n",
    "            param[1].requires_grad=False\n",
    "        for param in model.fc.parameters():\n",
    "            param[1].requires_grad=True\n",
    "        optimizer=optim.SGD(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "        \n",
    "\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    model=model.to(device)\n",
    "    print(f\"Training on {str(device)}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss_sum, test_loss_sum, train_acc_sum, n, batch_count = 0.0, 0.0, 0.0, 0, 0\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()  \n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()  \n",
    "            test_acc_sum, n2 = 0.0, 0\n",
    "            for X, y in test_iter:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                test_loss=loss_fn(model(X), y)\n",
    "                test_loss_sum+=test_loss.cpu().item()\n",
    "                test_acc_sum += (model(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                n2 += y.shape[0]\n",
    "        \n",
    "        print('epoch %d, train loss %.4f, test loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_loss_sum / batch_count, test_loss_sum / batch_count, train_acc_sum / n, test_acc_sum / n2))\n",
    "\n",
    "        train_losses.append(train_loss_sum / batch_count)\n",
    "        test_losses.append(test_loss_sum / batch_count)\n",
    "        train_accs.append(train_acc_sum / n)\n",
    "        test_accs.append(test_acc_sum / n2)\n",
    "\n",
    "\n",
    "train(model, train_iter, test_iter, epoches, loss_fn, 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis=[num for num in range(1, epoches+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, train_accs, color='r', label='train accuracy')\n",
    "plt.plot(x_axis, test_accs, color='g', label='test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_axis, train_losses, color='r', label='train loss')\n",
    "plt.plot(x_axis, test_losses, color='g', label='test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad0b274546ec867a14a2f05658beded19f1641156965d15cb2483469e4881a1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
